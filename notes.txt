16 Dec 2021

- Work over 2 weeks (no meetings)

- ADR files on GDrive that contain data
  - example: "feel weird" in dict
    but "feel a bit weird" in text
    -- want to be able to highlight these small modifers
- Side Effect database (multi-word expressions we can ignore for the time being)

  - Download these databases
  - Will download and create zip files to use them

- Term 2 Plan - single-word experimental task - goal: get a final weighted search score from multi-trie search - want to just prove that we can take typo and misspellings into account for finding matching words - redesign output for search results - start developing multi-word search approach - giving a few examples of how it looks - developing around these handful of examples - can use NLP tool to "lemmatize" words (e.g.
  feels, feel, feeling should all match one thing)
  lemmaization?
  ACTION ITEMS
- take a look at ADR files
  - come up with experimental tasks in GDrive
    for single word experiment
  - can we build one big dictionary?
  - clean up and keep only single word expressions
  - not all are in the same file formats
- clinical trial database
  - find
- benchmark 1 million short texts

  - saved trie as binary object

- construct tries and save them off as binary objects
  ! nice preprocessing step to save time in overall system

- start from interface

  - input text
  - upload file
  - pretend that search is happening
  - reference links
    - https://cogcomp.seas.upenn.edu/page/demos/
      [entities]
    - https://cogcomp.seas.upenn.edu/page/demo_view/MultipackageTS
      [multi language/tokenization demo]

- Term 3 Plan
  - multi-word evaluation task
    - multi-word expressions
  - snowmed-CT task

---

6 jan 2022

- weight similarity scores from results, after getting distinct set of results
  - provide interface to let user provide weights for the forest
- one approach -- weight each similiarity score (for each phonetic/non-phonetic repr)

  - and then avg them
  - MVP -- just weight all by 1
    - provide some interface to let users set weights themselves
      - future work?
    - "forest_similarity" as final score
  - also a way to set forest similarity threshold when producing "final" output
    - default to 0 -- all words from forest will be given
      - higher, then filter out results with scores lower

- design and run few experiments to evaluate single word NER

  - running exact matches (ED = 0)
    - compare execution times between exact match to similarity search
    - how much slower is this?
    - how much more complicated is this process?
      - weighting might be important for filtering correct results
    - can then show running this experiment with multiple tries, evaluate if there's a speed up (and magnitude)

- multi-word search !!

  - expressions will have more than one word (e.g. names of places, "New York", "Los Angeles")
  - e.g."Customer Experience Center"
    - generic words, but it's a proper name
      - what if not proper name -- instead the words are descriptive of some generic idea/concept
      - e.g. in text "Customer Center" -- do we annotate as the same place?
        - can be annotated, but will have a "word distance" -- not an exact match to "Customer _Experience_ Center"
          - words don't behave the same ways a letters
          - use length of words as a way to calculate "word distance" e.g. missing a short word ("of", "the", "in")
  - e.g. "Center for Customer Experience"
    - search: "Customer Experience Center"
      - missing "for" and order is incorrect
        - for is just a preposition -- smaller impact on "closeness" of match
          - how should we calculate word distance for this annotation?
            - one way: count diff of characters (24/27), or diff of words (3/4)
              - how to account for words out of order?
  - consider using frequency of word to adjust "word distance"
    e.g. words of high frequency that is missing, maybe we have it have less penalty
    - high frequency, less specifity -- some notion of calculating this

- which sentence tokenizer will we use?
- which sentense splitter will we use?
  - don't want to mix expressions in two different sentences
  - eventually make all available, give user option to choose which one they would like to use
  - at least one -> generally, the more tokens the better
- for each token, add each token into the forest as a single word
- build mapping between dictionary entries and words; will have multiple dictionaries

- first task: single word matching to possible multi-word entries in dictionary

  - don't want to deal with phonetic scores -- just want forest score
  - assume that forest score has been tuned to return good reuslts

- input is no longer a word; input will be plain text e.g. paragraphs, document, etc

  - text should be annotated with all available dictionaries

- multi-word search -- a wrapper around forest?

  - ~~forest -> multitrie?~~
  - forest -> ptrie? phonetrie? phonetrie?

  - forest will be multi-word search

- final, final evaluation
  - run MTsamples through clinical annotation tool
  - use same dictionary as tool in forest
  - compare results

---

13 jan 2022

- evalutation task; words are from FDA (e.g. drug names, etc)
- test set are words removed from this list
  - train and tuning is built from remaining words
    e.g. can get all words in test set by taking dictionary and
    removing words found in train and tuned
  - train also then has search words
    - some are exact matches
    - most are slight modifications
      (e.g. additional characters, removed characters, etc)

- why have training?
  - want to "learn" how to identify the correct word given that
    there are multiple results;
    based on scores find combination that's maximizes accuracy
  ! need to implement forest_similarity
  - because we know which word is our "truth" value
    we can save results and produce "match" value
    - binary value if the result word is the target word
      we're searching for
  * have to "produce" training set

- what are we doing with train set?
  * will be about ~100.000 rows
  - remove search + result from dataset
  - keep labels
  - keep ED, similarity, etc as features
  - then run logistic regression

- what do we do with logistic regression model?
  - input scores -> match prediction (confidence)
    - "forest_similarity"
  - what should we threshold this logistic model result at?
    ! with the tuning set
      - do same pre-processing as trainings set
      - don't have to train a new model
      - compute the ROC curve
        - optimize F1 score
    * don't worry about validation; seems like logistic regression
      won't overfit
        - increase number of interactions in case model doesn't converge
      - save model as .pkl object
- if we search a word that's not in the dictionary,
  expect really low scores for similarity and what not
  - these words will not be considered then
  ! if all words don't meet threshold are considred not found

* this will be an "imbalanced dataset"
  - 1 true examples for every 6-8 false examples

- when we have a dictinoary with no logistic regression trained
  just take average of each similarity score
  - when there's no logistic regression
  - then can just manually set a threshold

- three smaller tasks
  - for all of these, record search times for comparison
  ! pre task 1
    - exact match finding; ED = 0
      - create trie with all words in dictionary
      - search training set entirely, ED = 0
      * there's a chance by creating wrong word, it exists
        in the dictionary
        - record the number of times this happens
      ? what happens when typos in a word produce other valid words
        * seems like this doesn't ever happen (at least for this particular dictionary and query set)
      ? how does our system handle this
  ! pre task 2
    - search ED = 1, no phonetics
    - use jaro-winkler to rank results, then just take first results
    - calculate accuracy score; top rank resulting word if word is
      the one we're searching or not
    - calculate MRR (mean recipriocal rank), [0, 1]
      (sum (1 / rank of matching)) / n_searches
      - will be 0 if never find correct words, 1 when words
        are _always_ top ranked
  ! pre task 3
    - search ED = 2, no phonetics
    - use jaro-winkler to rank results
    - calculate accuracy score; top rank resulting word if word is
      the one we're searching or not
    - calculate MRR (mean recipriocal rank), [0, 1]
      (sum (1 / rank of matching)) / n_searches
      - will be 0 if never find correct words, 1 when words
        are _always_ top ranked

- tokenizers, lemma, etc -- multi-word search task...anything work
  done here will save time next term on work we have to do in this
  space

---

27 jan 2022

? how to incorporate model into search process
  - need to save out threshold learned from tuning set

---

03 feb 2022

- comparing f1 score from validation set to what?
- good opportunity to show how phonetic representations help with identifying words
- produce training set with no phonetic representation in tries
  - ed 2 or 3
- produce same dataset without any phonetic scores
  - compare f1 scores running logistic regression on this set
- produce same set with just string + double metaphone
- in essence, want to see how including different phonetic representations affects logistic regression model performance

---

10 feb 2022

! for swt, should count empty results as false
! run with only ED=0,1,2
! as edit distance increase
  - precision decrease
    - may not necessarily decrease
  - recall increase
  - this is what's expected
- w/o phonetics, expect lower recall, maybe higher precision
  - with phonetics, usually output more results
  - more candidate results means it's likely we have the
    correct word in the group
  - but, we'll likely lower precision
- discussing:
  - increaseing ED, does performance improve?
  - runtime tradeoff?
  - set up and design
  - setting up logistic regression model with filtering
- metamap -> identifies phrases in medical texts
  uses some ML methods
  - would like to try and get close to precision
  - simple method with similar search is able to
    to be competitive with ML methods (for this task)
- use FDA dataset and train a model
  is model able to work with other dictionaries
  - perhaps get better performance with a LR model tuned
    specifically for dictionary, but maybe we can still
    get reasonable performance

---

17 feb 2022

- calculate precision, recall, f1 from the test set
- query "SEARCH" from datasets
  - rank result set based on similarity
  - determine result is TP, FP, FN
  - working from provided train/val/test sets,
    not the ones generated by the script
- train LR model
  - manually tune LR threshold
  - LR is used to filter out low similarity

---

24 feb 2022

- two uses for evaluating test set
  - evaluating the performance of the logistic regression model
  - put the LR model in the ptrie to use the threshold and filter out results from search
- training LR model with higher ED _with phonetic representations included_
  - still open question on performance changes if we remove phonetic representations
? what is the best set up to optimize
- now have to evaluate the forest, searching for words
  - important to note that we can re-use LR classifier across forests
  - modifying forest (which tries to use, thresholds, etc) will affect f1 scores
    _when conducting searches on the forest_, but not the f1 score of the LR model
* split training of LR model and searching of forest
- create connection between LR and forest search
- save best threshold as part of it
- set threshold to 0.5, 0.75 and see how F1 score is affected
  * report this out

---

3 mar 2022

- using similarity and edit distance scores to build train and tuning sets
  - we can look at LR model to collect weights for each feature in the model
  - can figure out which phonetic representations has the most signal
  - what should we do with higher weighted phonetic representations
    - create forest with only tries that have high weights
      i.e. string representation + dmetaphone representation
    - just for tests
  - compare performance of forest with less tries and compare performance
  - in nasa dataset, expect a drop in f1 score
    - much messier data
  - lemmaization so that root of words are similar
    - SpaCy does this, so it can tokenize sentences
    - can use results without lemmaization to motivate using
      a lemmatizer during multi-word search
  - term 3 gameplan
    - run multiword search, expecting poor results i.e f1 score of .1-.2
    - then include lemmaization, which should really improve results
  - multiword task design
    - construct sentences such that we know what to expect what should
      be highlighted
    - how to calculate similarity scores with multi-word expressions

  - run edit distance 3 (only after optimizations)
  - add logging
    - log time it takes to construct trie
    - log time to search trie
    - log time to train LR model
      - include time it takes to tune
  - only using logistic regression for a subset of the NER task
    - domain specific tasks where data isn't available
      (e.g. annotated text to use as input)

11 apr 2022
- dataset generation and evaluation task, expect around thursday april 14
- for single word evaluation
  - created perturbations (e.g. missing character, added characters)
  - how to do this for multi-word expressions?
  - does this make sense for multi-word expressions?
    - will it "overlearn" on errors we introduce manually
  - will try this as a starting point
- same expression may have multiple CUIDs
  - we won't restrict this
  - final annotation -- should it give multiple CUIDs?
  - need to come up with two mappings
    - expression splitting to their words
    - expression to their CUIDs

- other task, splitting text into words
  - then search on each of the words
  - candidate words returned are similar words
  - need to find comparison of expression and CUIDs
  - at some point we need to "decide" which CUID to annotate an expression
  - going to have lots of potential combinations of scores and CUIDs
  - how do we determine which annotations go to which words
  - which words are connected to other words?
- capture what you need in each word
  - keep track of cuid, similarities, etc
  - make decision on "sliding window" on text
  - make this decision a function that we can call repeatedly for larger texts
- can test performance for larger piece of texts by concating a bunch of sample text
- expression edit distance?
  - maybe takes into account like similarity between words
  - different words?
  - etc
- can't process very large blocks of text at once
  - too many possible connections
  - too much memory
  - maybe we consider a limit on tokens
  - maybe we start "eliminating" potential phrases after we identify an expression
  - this may be problematic when we have expressions w/in expressions
- maybe consider differentiability of word
  - look at number of expressions and unique CUIDs
  - set of CUIDs will probably matter more
- implementation consideration
  - memoization; save queries already done so that we don't re-do searches
  - could save quite a bit since searches are very expensive operations

18 apr 2022

- switching between training and test sets?
- running single word match
  - training process with the logistic regression
  - input scores from the matching process
  - ran once for the drug names
- nasa data set; run training process on that dataset
  - from that training process, what are the weights from logistic regression
  - compare weights to FDA to NASA dataset
  * ? are those weights similar to each other
    ? can we use the same logistic regression model across multiple domains

- data set notes
  - removed entries with only 1 or 2 words
  - removed entries that aren't related to medical concepts, e.g. WAS, "I" as 1 instead of pronoun
  - MTSample data files
  - sentence may occur multiple times, but each sentence has different concepts
  - didn't want to have same sentence/same concepts split
  - didn't want concepts split between test/train
  - didn't want sentences to be split between test/train
  - very small number of non-overlapping material
    - but we can train on this small numnber (similar to "lower resource" domains)
    - 233 sentences, 263 concepts
  - give sentence and annotation
    - what is the format of this file

? what happens when we split train/test
  * also do this for NASA dataset
  - if they're all similar, why not average them?
  - or perhaps just use one that seems close to the average
  ? why do this
    - if we are working in low resource domains
      then we could argue that re-using past trained models
      is more resource efficient
    - as backup, create full dictionary of words to train new LR
      to compare results

- why not use the UML dataset to train?
  - we are considering "low resource" domains

- when reporting results will also want to give analysis on full
  and partial matches when reporting results
  - this is for sentence containing disease & syndrome
  - might have time to consider full dataset
  - 10 different semantic types (10 different data sets)
    * from here we can look at which types of phrases and domains
      the system performs better/worse on

- return from annotation
  - match words
  - candidate preferred (annotation)
  - candidate matched (annotation)
  - annotations will be denoted by character positions

- semantic_type (dictionary source)
  - can be used to split datasets for specific tasks

- will get one big file, each line is a JSON object
- will also get gazetteers for concept ids/phrases

! first CUID should be preferred annotation in case there are multiple
  expressions with the same CUID

  - f1 score for text matching
    - strict f1
      - match exactly start and end of an expression
    - "flexible" f1
      - whenever there's an overlapping, count as a match
      - may miss or add a word, but as long one character
        in the annotation, it's considered a matching
    - strict f1 will be lower

! start thinking about how to get start and end character positions
  for annotations
* aim for associated CUIs for each word output
  - develop some basic heuristics
  - since we don't have enough time to come up with some scheme
    for training some sort of regression

- for diagrams use powerpoint?
  - for figures, must not lose resolution (requirement by most publishers)

25 apr 2022

- dont want to deal with too many options per word
  - "heuristically" find a threshold for JW similarity
  - aim to deal with less than 5 possibilities
  - fix the number and run end to end
    - then start to experiment with adding LR and other things to the system
- in uml, one concept id shouldn't have two CUID for a given expression
- make simple decisions first
- work towards making a simple annotation first
  - one potential solution
    - is first token beginning of anything?
    - what is the end for each potential beginng of expression?
    - give a number to each candidate "beginning"
    - for every single beginning of expression, we need to find its end
? claiming dont have data to train model
  - produce synthetic data based on input dictinoary

* for each expression also store the tokens it splits into
* for each word also need to know
  - token information
    - expression determinstic score = how many expressions it appears in
      score = 1, only 1 expression
      score = 0, appears in all expression
    - cuid deterministic score = how many cuids it appears in
      1 = only in 1 cuid
      0 = appears in all CUIDs
  - also find position of word in found expression
- numbers for deciding if phrase begin/ends
  - come from expression, not CUIDs
- dont want to map CUID to token right away
  - want pair of expression cuid -- want to know what phrase which CUID comes from

- only think about beginning of expressions
  - and try to get this as accurate as possible
  - have to compare probability of being beginning of expression
  - compare possibilities of phrase to prior tokens
    - cant see same expression within same expression
  
- set task for now is to figure out beginning 
! propose how to make a decision based on those numbers to set a beginning of a expression
  - without machine learning
- how many tokens we will consider
  - use 2 for "word distance"
- how many of those 7 have those CUIDs
- valid tokens -- how many tokens associated with the same expression (in the tokens were looking at)

- if this is all we do, then we can find valuable of beginning of expression f1 score

- come up with notion of word distance
  - compare expression of 6 tokens vs 7 tokens
  - at least 1 token token, so at least WD=1
  - if tokens are unrelated, add 1
  - tokens that dont follow order, add 1

2 may 2022
  - word distance implementation by word oracle?
  - length of input token
  - simlarity score between input token and similar search result
  * need to come up with a few numbers
    - and then just try to do something simple
    - since we only have so much time left
  * find beginning word of a expression and related concept id
    - need to look ahead at a handful of tokens
    - 5 tokens, arbitrary because we're using ED 2
    - might need to train model per CUID
    - when a token has any CUIDs, look at token surrounding it and need to potentially train
      LR that identifies whether a particular CUID starts at this token position
      - numbers corresponds with a token being a beginning of a sentence
      * one LR model but ask it multiple CUIDs
        ! need to evaluate multiple CUIDs during the search process
      - train model on IMDB database and see if we can transfer model across domains
      - in theory, no training data, just dictionary
  - dont need to highlight entire expression, just find "beginning word of CUID"
  - we know where expression starts in most of cases, not all
    - some starting tokens have value -1 -- model error?
  - no reason to have two beginnings side by side
  - partial correctness only when annotation is nearby a ground truth
  * propose some way to evaluate the model
  - figure out length of expression -- how many tokens in this expression, including some
    notion of "fuzziness"/error?
  - entirely possible that word distance is not needed
  - if the expression is longer than our sliding window, then it doesn't matter
    - only looking for beginning of expression
  - for beginning and end add negative numbers to the sliding window
    - learn that it's at the end/beginning of the document (-1, -2, etc based on how many tokens out)

  - creating dataset
    - only starting with dictionary
    - number of expressions from the dictionary in the text
      - 0-5 
    - from expressions that we have:
      - induce some minor edit distance in the words
      - say 5%-10% of tokens to remove, duplicate, or insert character
        - only do one operation
        - just to create noise
      - in 5% of expressions, remove word, duplicate word, insert word
        - can insert a random word from the dictionary
        - only do one operation, in one of the words in the expressions
        - in very low %, like 1% or 0.5% add abbreviations of tokens that are > 5 characters -- just an idea
      - then conconate expressions with random words in between from the dictionary
        - put 0-5 words inbetween
      - if words randomly generate end up being an expression, mark it as a false positive
        - an expression was generated by chance, but not one we wanted/should? identify
      - constructing a dataset, we have a label
      - create data which just has token, index of token, cuid, numbers

    - split dataset into two groups, half and half
    - only use "training set" to build model
    - create sentence with first 50%, then create sentences with other 50%
      - only use one of these sets for training
    - only learning from half of dictionary only
    - create 1 thousand sentences
      - should get more than enough data

  - calc det scores
    - # of different CUIDs
    - # of different expressions
    - expressions more than CUID
    - 1 - (x / len(expressions[]))
  - plot histogram of det score for each token

22 june 2022
  - what words to use as filler words
    ! use combination of all three of these together
    - stop words
      - a, and, the, without, from, to since, until -- don't have specific meaning
    - words from other expressions (expose existing dictionary)
    - select randomly 1000 words from wordnet
  - for labels, use tokens to label
    - tokenizer used will affect this, make sure to document this and is consistent throughout the system
  - make sure all the same expressions that correspond with one CUID is in either train or test
    - don't mix expressions with same CUID in train/test datasets
    - 70:15:15 or 60:20:20 - training, tuning, test
  - include other CUIDs when searching from forest -- include these as false,
    similar to training set construction for single-word task
  - when building training set, add words not related to the expression into the expression
    - introduce noise into existing expressions
    - get one of words from expression
      - pad with stop words, random words, etc
    - full true-negative example
      - we want these examples otherwise the system will learn that all sentences must always have an expression
    - random words we get from wordnet should not correspond to an expression
    - internal notation agreement
      - metamap "silver standard"
      - don't have ground truth notations
      - this is to replace the use of an f1 score for measuring performance of our system

27 june 2022
  - NASA shared task
    - annotations which will be validated by shared task
    - making solution works for beginning and internal tokens
    - even with poor results, we can analyze why the model performs poorly
      - also difference between FOREST and top performing models
    - use annotations in training set to build up dictionary and to generate
      annotations and figure out "type" of expression
    - won't be able to reach disambiguation of the same term
      in multiple contexts
    ! dont use Beginning/Internal on dataset when constructing dictionary
  - with clinical context, don't have annotations
  - instead of learning the end token, can learn internal tokens (+ end)
    - try and do both end and internal tokens together
      - have to figure out other tokens in the same sentence
      - either figuring out the end notation
      - or learning which tokens are still a part of the expression
      - all 3 are separate tasks
    - it may be easier to learn internal tokens
      - if we learn internal token first, can then use this to learn
        beginning and end tokens for expression
      - may be the case that just beginning and end token should be enough
      